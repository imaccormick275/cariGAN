{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oC73tcNw-_oL"
   },
   "source": [
    "# CycleGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VmDXIY2A_UFp"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "-7Yyy6nx75u4",
    "outputId": "f1ea2847-d104-486e-f916-0685fd2db0fe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PYOOVTKBIuR-"
   },
   "source": [
    "### Utils.py\n",
    "\n",
    "Utility functions to process data set: load test and train data sets, read images, save images, crop images, transform/inverse transform images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sfleHQhmItFz"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Some codes from https://github.com/Newmu/dcgan_code\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "import math\n",
    "import pprint\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "import copy\n",
    "#try:\n",
    "    #_imread = scipy.misc.imread\n",
    "#except AttributeError:\n",
    "from imageio import imread as _imread\n",
    "from imageio import imwrite as _imwrite\n",
    "#from PIL import Image as PILImage\n",
    "from skimage import transform as skimage_transform\n",
    "\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "get_stddev = lambda x, k_h, k_w: 1/math.sqrt(k_w*k_h*x.get_shape()[-1])\n",
    "\n",
    "# -----------------------------\n",
    "# new added functions for cyclegan\n",
    "class ImagePool(object):\n",
    "    def __init__(self, maxsize=50):\n",
    "        self.maxsize = maxsize\n",
    "        self.num_img = 0\n",
    "        self.images = []\n",
    "\n",
    "    def __call__(self, image):\n",
    "        if self.maxsize <= 0:\n",
    "            return image\n",
    "        if self.num_img < self.maxsize:\n",
    "            self.images.append(image)\n",
    "            self.num_img += 1\n",
    "            return image\n",
    "        if np.random.rand() > 0.5:\n",
    "            idx = int(np.random.rand()*self.maxsize)\n",
    "            tmp1 = copy.copy(self.images[idx])[0]\n",
    "            self.images[idx][0] = image[0]\n",
    "            idx = int(np.random.rand()*self.maxsize)\n",
    "            tmp2 = copy.copy(self.images[idx])[1]\n",
    "            self.images[idx][1] = image[1]\n",
    "            return [tmp1, tmp2]\n",
    "        else:\n",
    "            return image\n",
    "\n",
    "def load_test_data(image_path, fine_size=256):\n",
    "    img = imread(image_path)\n",
    "    img = skimage_transform.resize(img, (fine_size, fine_size))\n",
    "    img = img/127.5 - 1\n",
    "    return img\n",
    "\n",
    "def load_train_data(image_path, load_size=286, fine_size=256, is_testing=False):\n",
    "    img_A = imread(image_path[0])\n",
    "    img_B = imread(image_path[1])\n",
    "    if not is_testing:\n",
    "        img_A = skimage_transform.resize(img_A, (load_size, load_size))\n",
    "        img_B = skimage_transform.resize(img_B, (load_size, load_size))\n",
    "\n",
    "        h1 = int(np.ceil(np.random.uniform(1e-2, load_size-fine_size)))\n",
    "        w1 = int(np.ceil(np.random.uniform(1e-2, load_size-fine_size)))\n",
    "        img_A = img_A[h1:h1+fine_size, w1:w1+fine_size]\n",
    "        img_B = img_B[h1:h1+fine_size, w1:w1+fine_size]\n",
    "\n",
    "        if np.random.random() > 0.5:\n",
    "            img_A = np.fliplr(img_A)\n",
    "            img_B = np.fliplr(img_B)\n",
    "    else:\n",
    "        img_A = skimage_transform.resize(img_A, (fine_size, fine_size))\n",
    "        img_B = skimage_transform.resize(img_B, (fine_size, fine_size))\n",
    "\n",
    "    img_A = img_A/127.5 - 1.\n",
    "    img_B = img_B/127.5 - 1.\n",
    "\n",
    "    img_AB = np.concatenate((img_A, img_B), axis=2)\n",
    "    # img_AB shape: (fine_size, fine_size, input_c_dim + output_c_dim)\n",
    "    return img_AB\n",
    "\n",
    "# -----------------------------\n",
    "\n",
    "def get_image(image_path, image_size, is_crop=True, resize_w=64, is_grayscale = False):\n",
    "    return transform(imread(image_path, is_grayscale), image_size, is_crop, resize_w)\n",
    "\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(inverse_transform(images), size, image_path)\n",
    "\n",
    "def imread(path, is_grayscale = False):\n",
    "    if (is_grayscale):\n",
    "        return _imread(path, as_gray=True).astype(np.float)\n",
    "    else:\n",
    "        return _imread(path, pilmode='RGB').astype(np.float)\n",
    "\n",
    "def merge_images(images, size):\n",
    "    return inverse_transform(images)\n",
    "\n",
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1], 3))\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "\n",
    "    return img\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    return _imwrite(path, merge(images, size))\n",
    "\n",
    "def center_crop(x, crop_h, crop_w,\n",
    "                resize_h=64, resize_w=64):\n",
    "  if crop_w is None:\n",
    "    crop_w = crop_h\n",
    "  h, w = x.shape[:2]\n",
    "  j = int(round((h - crop_h)/2.))\n",
    "  i = int(round((w - crop_w)/2.))\n",
    "  return skimage_transform.resize(\n",
    "      x[j:j+crop_h, i:i+crop_w], (resize_h, resize_w))\n",
    "\n",
    "def transform(image, npx=64, is_crop=True, resize_w=64):\n",
    "    # npx : # of pixels width/height of image\n",
    "    if is_crop:\n",
    "        cropped_image = center_crop(image, npx, resize_w=resize_w)\n",
    "    else:\n",
    "        cropped_image = image\n",
    "    return np.array(cropped_image)/127.5 - 1.\n",
    "\n",
    "def inverse_transform(images):\n",
    "    return (images+1.)/2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_TvRLybkAO-Y"
   },
   "source": [
    "### ops.py\n",
    "\n",
    "Defines operations which are later to be used to define generator and discrimnator networks. Includes, batch_norm, instance_norm, conv2d, deconv2d, lrelu and linear functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "6q6ik_s7AQh6",
    "outputId": "03561034-d856-4c15-b220-33901f176c32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "# from utils import *\n",
    "\n",
    "#####Â New Helper Functions\n",
    "\n",
    "# weight and bais wrappers\n",
    "def weight_variable(name, shape):\n",
    "    \"\"\"\n",
    "    Create a weight variable with appropriate initialization\n",
    "    :param name: weight name\n",
    "    :param shape: weight shape\n",
    "    :return: initialized weight variable\n",
    "    \"\"\"\n",
    "    initer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "    return tf.get_variable('W_' + name,\n",
    "                           dtype=tf.float32,\n",
    "                           shape=shape,\n",
    "                           initializer=initer)\n",
    "\n",
    "def bias_variable(name, shape):\n",
    "    \"\"\"\n",
    "    Create a bias variable with appropriate initialization\n",
    "    :param name: bias variable name\n",
    "    :param shape: bias variable shape\n",
    "    :return: initialized bias variable\n",
    "    \"\"\"\n",
    "    initial = tf.constant(0., shape=shape, dtype=tf.float32)\n",
    "    return tf.get_variable('b_' + name,\n",
    "                           dtype=tf.float32,\n",
    "                           initializer=initial)\n",
    " \n",
    "\n",
    "def fc_layer(x, num_units, name, use_relu=True):\n",
    "    \"\"\"\n",
    "    Create a fully-connected layer\n",
    "    :param x: input from previous layer\n",
    "    :param num_units: number of hidden units in the fully-connected layer\n",
    "    :param name: layer name\n",
    "    :param use_relu: boolean to add ReLU non-linearity (or not)\n",
    "    :return: The output array\n",
    "    \"\"\"\n",
    "    in_dim = x.get_shape()[1]\n",
    "    W = weight_variable(name, shape=[in_dim, num_units])\n",
    "    b = bias_variable(name, [num_units])\n",
    "    layer = tf.matmul(x, W)\n",
    "    layer += b\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "    return layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hQj9hod8AJIl"
   },
   "source": [
    "###Â module.py\n",
    "\n",
    "Used to define descriminator network, generator network (with and without residual block), and three loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "liipJDgJAL8v"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import tensorflow as tf\n",
    "# from ops import *\n",
    "# from utils import *\n",
    "\n",
    "# discriminator network\n",
    "def discriminator(pca_vector, options, reuse=False, name=\"discriminator\"):\n",
    "\n",
    "    with tf.variable_scope(name):\n",
    "        # image is 256 x 256 x input_c_dim\n",
    "        if reuse:\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "        else:\n",
    "            assert tf.get_variable_scope().reuse is False\n",
    "            \n",
    "        output_dimension = pca_vector.shape[1]\n",
    "\n",
    "        h0 = fc_layer(pca_vector, 128, name='d_h0', use_relu=True)\n",
    "        # h0 is (128 x 128 x self.df_dim)\n",
    "        h1 = fc_layer(h0, 128, name='d_h1', use_relu=True)\n",
    "        # h1 is (64 x 64 x self.df_dim*2)\n",
    "        h2 = fc_layer(h1, 128, name='d_h2', use_relu=True)\n",
    "        # h2 is (32x 32 x self.df_dim*4)\n",
    "        h3 = fc_layer(h2, 128, name='d_h3', use_relu=True)\n",
    "        # h3 is (32 x 32 x self.df_dim*8)\n",
    "        h4 = fc_layer(h3, 128, name='d_h4', use_relu=True)\n",
    "        # h4 is (32 x 32 x 1)\n",
    "        return h4\n",
    "\n",
    "# generator network without residual block\n",
    "def generator(pca_vector, options, reuse=False, name=\"generator\"):\n",
    "\n",
    "    with tf.variable_scope(name):\n",
    "        # pca_vector is ((k=32 < m=number of coordinates x 2) x input_c_dim)\n",
    "        if reuse:\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "        else:\n",
    "            assert tf.get_variable_scope().reuse is False\n",
    "            \n",
    "        output_dimension = pca_vector.shape[1]\n",
    "\n",
    "        # pca_vector is ((k=32 < m=number of coordinates x 2) x input_c_dim)\n",
    "        e1 = fc_layer(pca_vector, 64, name='g_e1', use_relu=True)\n",
    "        # e1 is (128 x 128 x self.gf_dim) 1048576\n",
    "        e2 = fc_layer(e1, 128, name='g_e2', use_relu=True)\n",
    "        # e2 is (64 x 64 x self.gf_dim*2) 524288\n",
    "        e3 = fc_layer(e2, 256, name='g_e3', use_relu=True)\n",
    "        # e3 is (32 x 32 x self.gf_dim*4) 262144\n",
    "        e4 = fc_layer(e3, 512, name='g_e4', use_relu=True)\n",
    "        # e4 is (16 x 16 x self.gf_dim*8) 131072\n",
    "        e5 = fc_layer(e4, 256, name='g_e5', use_relu=True)\n",
    "        # e5 is (8 x 8 x self.gf_dim*8) 32768\n",
    "        e6 = fc_layer(e5, 128, name='g_e6', use_relu=True)\n",
    "        # e6 is (4 x 4 x self.gf_dim*8) 8192\n",
    "        e7 = fc_layer(e6, 64, name='g_e7', use_relu=True)\n",
    "        # e7 is (2 x 2 x self.gf_dim*8) 2048\n",
    "        e8 = fc_layer(e7, output_dimension, name='g_e8', use_relu=False)\n",
    "        # e8 is (1 x 1 x self.gf_dim*8) 512\n",
    "\n",
    "        return e8\n",
    "\n",
    "    \n",
    "# loss function\n",
    "def abs_criterion(in_, target):\n",
    "    return tf.reduce_mean(tf.abs(in_ - target))\n",
    "\n",
    "## loss function\n",
    "def mae_criterion(in_, target):\n",
    "    return tf.reduce_mean((in_-target)**2)\n",
    "\n",
    "## loss function\n",
    "def sce_criterion(logits, labels):\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "\n",
    "\n",
    "## loss function\n",
    "def cosine_distance(real_1, real_2, fake_2):\n",
    "    # real_1 minus mean of real_1 for all y in Y\n",
    "    \n",
    "    # test = tf.fill(tf.shape(real_1)[:-1], 1)\n",
    "    # test = tf.ones(real_1.shape[0])\n",
    "    A = real_1 - tf.matmul(tf.fill(tf.shape(real_1)[:-1], 1).reshape(-1,1), real_1.mean(axis=0).reshape(1,-1))\n",
    "\n",
    "    # fake_2 minus mean of X for all y in Y\n",
    "    B = fake_2 - tf.matmul(tf.fill(tf.shape(real_1)[:-1], 1).reshape(-1,1), real_2.mean(axis=0).reshape(1,-1))\n",
    "\n",
    "    # calculate cosine distance matrix\n",
    "    cosine_distance_matrix = tf.multiply(tf.ones((real_1.shape[0],1)) - cosine_similarity(A, B, dense_output=True),\n",
    "                                         tf.eye(real_1.shape[0]))\n",
    "    \n",
    "    # calculate loss\n",
    "    cos_dist = cosine_distance_matrix.sum()\n",
    "    \n",
    "    return cos_dist\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9PCPZAXGAFPu"
   },
   "source": [
    "###Â Model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UsfS4l5BAAfd"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import time\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "\n",
    "#Â from module import *\n",
    "# from utils import *\n",
    "\n",
    "# COMMENT: \n",
    "class cyclegan(object):\n",
    "    def __init__(self, sess, args):\n",
    "        # COMMENT:\n",
    "        self.sess = sess\n",
    "        self.batch_size = args.batch_size\n",
    "        \n",
    "        self.num_components = args.pca_components\n",
    "\n",
    "        self.L1_lambda = args.L1_lambda\n",
    "        self.dataset_dir = args.dataset_dir\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "\n",
    "        # COMMENT:\n",
    "        if args.use_lsgan:\n",
    "            self.criterionGAN = mae_criterion\n",
    "        else:\n",
    "            self.criterionGAN = sce_criterion\n",
    "\n",
    "        # COMMENT:\n",
    "        OPTIONS = namedtuple('OPTIONS', 'batch_size image_size \\\n",
    "                              gf_dim df_dim is_training')\n",
    "        self.options = OPTIONS._make((args.batch_size, args.pca_components,\n",
    "                                      args.ngf, args.ndf,\n",
    "                                      args.phase == 'train'))\n",
    "        \n",
    "        # COMMENT:\n",
    "        self._build_model()\n",
    "        \n",
    "        # COMMENT:\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        # COMMENT:\n",
    "        self.pool = ImagePool(args.max_size)\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        COMMENT:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # COMMENT:\n",
    "        self.real_A = tf.placeholder(tf.float32,\n",
    "                                     [None, self.num_components],\n",
    "                                     name='real_A')\n",
    "        self.real_B= tf.placeholder(tf.float32,\n",
    "                                    [None, self.num_components],\n",
    "                                    name='real_B')\n",
    "\n",
    "        # COMMENT:\n",
    "        self.fake_B = self.generator(self.real_A, self.options, False, name=\"generatorA2B\")\n",
    "        self.fake_A_ = self.generator(self.fake_B, self.options, False, name=\"generatorB2A\")\n",
    "        self.fake_A = self.generator(self.real_B, self.options, True, name=\"generatorB2A\")\n",
    "        self.fake_B_ = self.generator(self.fake_A, self.options, True, name=\"generatorA2B\")\n",
    "\n",
    "        # COMMENT:\n",
    "        self.DB_fake = self.discriminator(self.fake_B, self.options, reuse=False, name=\"discriminatorB\")\n",
    "        self.DA_fake = self.discriminator(self.fake_A, self.options, reuse=False, name=\"discriminatorA\")\n",
    "\n",
    "\n",
    "        # COMMENT:\n",
    "        self.g_loss_a2b = self.criterionGAN(self.DB_fake, tf.ones_like(self.DB_fake)) \\\n",
    "            + self.L1_lambda * abs_criterion(self.real_A, self.fake_A_) \\\n",
    "            + self.L1_lambda * abs_criterion(self.real_B, self.fake_B_)\n",
    "        self.g_loss_b2a = self.criterionGAN(self.DA_fake, tf.ones_like(self.DA_fake)) \\\n",
    "            + self.L1_lambda * abs_criterion(self.real_A, self.fake_A_) \\\n",
    "            + self.L1_lambda * abs_criterion(self.real_B, self.fake_B_)\n",
    "        \n",
    "        self.char_loss_a2b = cosine_distance(self.real_A, self.real_B, self.fake_B)\n",
    "        self.char_loss_b2a = cosine_distance(self.real_B, self.real_A, self.fake_A)\n",
    "        \n",
    "        self.g_loss = self.criterionGAN(self.DA_fake, tf.ones_like(self.DA_fake)) \\\n",
    "            + self.criterionGAN(self.DB_fake, tf.ones_like(self.DB_fake)) \\\n",
    "            + self.L1_lambda * abs_criterion(self.real_A, self.fake_A_) \\\n",
    "            + self.L1_lambda * abs_criterion(self.real_B, self.fake_B_) \\\n",
    "            + self.char_loss_a2b \\\n",
    "            + self.char_loss_b2a\n",
    "        \n",
    "        \n",
    "        # COMMENT:\n",
    "        self.fake_A_sample = tf.placeholder(tf.float32,\n",
    "                                            [None, self.num_components], name='fake_A_sample')\n",
    "        self.fake_B_sample = tf.placeholder(tf.float32,\n",
    "                                            [None, self.num_components], name='fake_B_sample')\n",
    "\n",
    "        # COMMENT:\n",
    "        self.DB_real = self.discriminator(self.real_B, self.options, reuse=True, name=\"discriminatorB\")\n",
    "        self.DA_real = self.discriminator(self.real_A, self.options, reuse=True, name=\"discriminatorA\")\n",
    "        self.DB_fake_sample = self.discriminator(self.fake_B_sample, self.options, reuse=True, name=\"discriminatorB\")\n",
    "        self.DA_fake_sample = self.discriminator(self.fake_A_sample, self.options, reuse=True, name=\"discriminatorA\")\n",
    "\n",
    "        # COMMENT:\n",
    "        self.db_loss_real = self.criterionGAN(self.DB_real, tf.ones_like(self.DB_real))\n",
    "        self.db_loss_fake = self.criterionGAN(self.DB_fake_sample, tf.zeros_like(self.DB_fake_sample))\n",
    "        self.db_loss = (self.db_loss_real + self.db_loss_fake) / 2\n",
    "        self.da_loss_real = self.criterionGAN(self.DA_real, tf.ones_like(self.DA_real))\n",
    "        self.da_loss_fake = self.criterionGAN(self.DA_fake_sample, tf.zeros_like(self.DA_fake_sample))\n",
    "        self.da_loss = (self.da_loss_real + self.da_loss_fake) / 2\n",
    "        self.d_loss = self.da_loss + self.db_loss\n",
    "\n",
    "        # COMMENT:\n",
    "        self.g_loss_a2b_sum = tf.summary.scalar(\"g_loss_a2b\", self.g_loss_a2b)\n",
    "        self.g_loss_b2a_sum = tf.summary.scalar(\"g_loss_b2a\", self.g_loss_b2a)\n",
    "        self.g_loss_sum = tf.summary.scalar(\"g_loss\", self.g_loss)\n",
    "        # COMMENT:\n",
    "        self.g_sum = tf.summary.merge([self.g_loss_a2b_sum, self.g_loss_b2a_sum, self.g_loss_sum])\n",
    "\n",
    "        # COMMENT:\n",
    "        self.db_loss_sum = tf.summary.scalar(\"db_loss\", self.db_loss)\n",
    "        self.da_loss_sum = tf.summary.scalar(\"da_loss\", self.da_loss)\n",
    "        self.d_loss_sum = tf.summary.scalar(\"d_loss\", self.d_loss)\n",
    "        self.db_loss_real_sum = tf.summary.scalar(\"db_loss_real\", self.db_loss_real)\n",
    "        self.db_loss_fake_sum = tf.summary.scalar(\"db_loss_fake\", self.db_loss_fake)\n",
    "        self.da_loss_real_sum = tf.summary.scalar(\"da_loss_real\", self.da_loss_real)\n",
    "        self.da_loss_fake_sum = tf.summary.scalar(\"da_loss_fake\", self.da_loss_fake)\n",
    "\n",
    "        # COMMENT:\n",
    "        self.d_sum = tf.summary.merge(\n",
    "            [self.da_loss_sum, self.da_loss_real_sum, self.da_loss_fake_sum,\n",
    "             self.db_loss_sum, self.db_loss_real_sum, self.db_loss_fake_sum,\n",
    "             self.d_loss_sum]\n",
    "        )\n",
    "\n",
    "        \n",
    "        # COMMENT:\n",
    "        self.test_A = tf.placeholder(tf.float32,\n",
    "                                     [None, self.num_components], name='test_A')\n",
    "        self.test_B = tf.placeholder(tf.float32,\n",
    "                                     [None, self.num_components], name='test_B')\n",
    "        self.testB = self.generator(self.test_A, self.options, True, name=\"generatorA2B\")\n",
    "        self.testA = self.generator(self.test_B, self.options, True, name=\"generatorB2A\")\n",
    "\n",
    "        t_vars = tf.trainable_variables()\n",
    "\n",
    "        \n",
    "        # COMMENT:\n",
    "        self.d_vars = [var for var in t_vars if 'discriminator' in var.name]\n",
    "        self.g_vars = [var for var in t_vars if 'generator' in var.name]\n",
    "        for var in t_vars: print(var.name)\n",
    "\n",
    "    def train(self, args):\n",
    "        \"\"\"Train cyclegan\"\"\"\n",
    "        # COMMENT:\n",
    "        self.lr = tf.placeholder(tf.float32, None, name='learning_rate')\n",
    "\n",
    "        # COMMENT:\n",
    "        self.d_optim = tf.train.AdamOptimizer(self.lr, beta1=args.beta1) \\\n",
    "            .minimize(self.d_loss, var_list=self.d_vars)\n",
    "        self.g_optim = tf.train.AdamOptimizer(self.lr, beta1=args.beta1) \\\n",
    "            .minimize(self.g_loss, var_list=self.g_vars)\n",
    "\n",
    "        # COMMENT:\n",
    "        init_op = tf.global_variables_initializer()\n",
    "\n",
    "        # COMMENT:\n",
    "        self.sess.run(init_op)\n",
    "\n",
    "        # COMMENT:\n",
    "        self.writer = tf.summary.FileWriter(\"./logs\", self.sess.graph)\n",
    "\n",
    "        # COMMENT:\n",
    "        counter = 1\n",
    "        start_time = time.time()\n",
    "\n",
    "        # COMMENT:\n",
    "        if args.continue_train:\n",
    "            if self.load(args.checkpoint_dir):\n",
    "                print(\" [*] Load SUCCESS\")\n",
    "            else:\n",
    "                print(\" [!] Load failed...\")\n",
    "\n",
    "        # COMMENT:\n",
    "        for epoch in range(args.epoch):\n",
    "\n",
    "            # COMMENT:\n",
    "            dataA = glob('./datasets/{}/*.*'.format(self.dataset_dir + '/trainA'))\n",
    "            dataB = glob('./datasets/{}/*.*'.format(self.dataset_dir + '/trainB'))\n",
    "            np.random.shuffle(dataA)\n",
    "            np.random.shuffle(dataB)\n",
    "            batch_idxs = min(min(len(dataA), len(dataB)), args.train_size) // self.batch_size\n",
    "\n",
    "            # COMMENT:\n",
    "            lr = args.lr if epoch < args.epoch_step else args.lr*(args.epoch-epoch)/(args.epoch-args.epoch_step)\n",
    "\n",
    "            # COMMENT:\n",
    "            for idx in range(0, batch_idxs):\n",
    "                # COMMENT:\n",
    "                batch_files = list(zip(dataA[idx * self.batch_size:(idx + 1) * self.batch_size],\n",
    "                                       dataB[idx * self.batch_size:(idx + 1) * self.batch_size]))\n",
    "\n",
    "                # COMMENT:\n",
    "                batch_images = [load_train_data(batch_file, args.load_size, args.pca_components) for batch_file in batch_files]\n",
    "\n",
    "                # COMMENT:\n",
    "                batch_images = np.array(batch_images).astype(np.float32)\n",
    "\n",
    "                # COMMENT:\n",
    "                fake_A, fake_B, _, summary_str = self.sess.run(\n",
    "                    [self.fake_A, self.fake_B, self.g_optim, self.g_sum],\n",
    "                    feed_dict={self.real_data: batch_images, \n",
    "                               self.lr: lr})\n",
    "                self.writer.add_summary(summary_str, counter)\n",
    "                [fake_A, fake_B] = self.pool([fake_A, fake_B])\n",
    "\n",
    "                # COMMENT:\n",
    "                _, summary_str = self.sess.run(\n",
    "                    [self.d_optim, self.d_sum],\n",
    "                    feed_dict={self.real_data: batch_images,\n",
    "                               self.fake_A_sample: fake_A,\n",
    "                               self.fake_B_sample: fake_B,\n",
    "                               self.lr: lr})\n",
    "                self.writer.add_summary(summary_str, counter)\n",
    "\n",
    "                # COMMENT:\n",
    "                counter += 1\n",
    "                print((\"Epoch: [%2d] [%4d/%4d] time: %4.4f\" % (\n",
    "                    epoch, idx, batch_idxs, time.time() - start_time)))\n",
    "\n",
    "                # COMMENT:\n",
    "                if np.mod(counter, args.print_freq) == 1:\n",
    "                    self.sample_model(args.sample_dir, epoch, idx)\n",
    "\n",
    "                # COMMENT:\n",
    "                if np.mod(counter, args.save_freq) == 2:\n",
    "                    self.save(args.checkpoint_dir, counter)\n",
    "\n",
    "    def save(self, checkpoint_dir, step):\n",
    "        model_name = \"cyclegan.model\"\n",
    "        model_dir = \"%s_%s\" % (self.dataset_dir, self.num_components)\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "        self.saver.save(self.sess,\n",
    "                        os.path.join(checkpoint_dir, model_name),\n",
    "                        global_step=step)\n",
    "\n",
    "    def load(self, checkpoint_dir):\n",
    "        print(\" [*] Reading checkpoint...\")\n",
    "\n",
    "        model_dir = \"%s_%s\" % (self.dataset_dir, self.num_components)\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "            self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def sample_model(self, sample_dir, epoch, idx):\n",
    "        dataA = glob('./datasets/{}/*.*'.format(self.dataset_dir + '/testA'))\n",
    "        dataB = glob('./datasets/{}/*.*'.format(self.dataset_dir + '/testB'))\n",
    "        np.random.shuffle(dataA)\n",
    "        np.random.shuffle(dataB)\n",
    "        batch_files = list(zip(dataA[:self.batch_size], dataB[:self.batch_size]))\n",
    "        sample_images = [load_train_data(batch_file, is_testing=True) for batch_file in batch_files]\n",
    "        sample_images = np.array(sample_images).astype(np.float32)\n",
    "\n",
    "        fake_A, fake_B = self.sess.run(\n",
    "            [self.fake_A, self.fake_B],\n",
    "            feed_dict={self.real_data: sample_images}\n",
    "        )\n",
    "        save_images(fake_A, [self.batch_size, 1],\n",
    "                    './{}/A_{:02d}_{:04d}.jpg'.format(sample_dir, epoch, idx))\n",
    "        save_images(fake_B, [self.batch_size, 1],\n",
    "                    './{}/B_{:02d}_{:04d}.jpg'.format(sample_dir, epoch, idx))\n",
    "\n",
    "    def test(self, args):\n",
    "        \"\"\"Test cyclegan\"\"\"\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        self.sess.run(init_op)\n",
    "        if args.which_direction == 'AtoB':\n",
    "            sample_files = glob('./datasets/{}/*.*'.format(self.dataset_dir + '/testA'))\n",
    "        elif args.which_direction == 'BtoA':\n",
    "            sample_files = glob('./datasets/{}/*.*'.format(self.dataset_dir + '/testB'))\n",
    "        else:\n",
    "            raise Exception('--which_direction must be AtoB or BtoA')\n",
    "\n",
    "        if self.load(args.checkpoint_dir):\n",
    "            print(\" [*] Load SUCCESS\")\n",
    "        else:\n",
    "            print(\" [!] Load failed...\")\n",
    "\n",
    "        # write html for visual comparison\n",
    "        index_path = os.path.join(args.test_dir, '{0}_index.html'.format(args.which_direction))\n",
    "        index = open(index_path, \"w\")\n",
    "        index.write(\"<html><body><table><tr>\")\n",
    "        index.write(\"<th>name</th><th>input</th><th>output</th></tr>\")\n",
    "\n",
    "        out_var, in_var = (self.testB, self.test_A) if args.which_direction == 'AtoB' else (\n",
    "            self.testA, self.test_B)\n",
    "\n",
    "        for sample_file in sample_files:\n",
    "            print('Processing image: ' + sample_file)\n",
    "            sample_image = [load_test_data(sample_file, args.pca_components)]\n",
    "            sample_image = np.array(sample_image).astype(np.float32)\n",
    "            image_path = os.path.join(args.test_dir,\n",
    "                                      '{0}_{1}'.format(args.which_direction, os.path.basename(sample_file)))\n",
    "            fake_img = self.sess.run(out_var, feed_dict={in_var: sample_image})\n",
    "            save_images(fake_img, [1, 1], image_path)\n",
    "            index.write(\"<td>%s</td>\" % os.path.basename(image_path))\n",
    "            index.write(\"<td><img src='%s'></td>\" % (sample_file if os.path.isabs(sample_file) else (\n",
    "                '..' + os.path.sep + sample_file)))\n",
    "            index.write(\"<td><img src='%s'></td>\" % (image_path if os.path.isabs(image_path) else (\n",
    "                '..' + os.path.sep + image_path)))\n",
    "            index.write(\"</tr>\")\n",
    "        index.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V2Lu6dyB_7a6"
   },
   "source": [
    "### main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "colab_type": "code",
    "id": "cg_luzBzEX8A",
    "outputId": "a60e56ae-e2c7-4291-d788-84125987d488"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c69d4e789517>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mtfconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_growth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtfconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcyclegan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;31m#model.train(args)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m#if args.phase == 'train' \\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-b9b8dee086cb>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sess, args)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# IAIN: if you create an instance of class cycleGAN build model is automatically called\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-b9b8dee086cb>\u001b[0m in \u001b[0;36m_build_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_loss_b2a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterionGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDA_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDA_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL1_lambda\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mabs_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_A_\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL1_lambda\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mabs_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_B_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar_loss_a2b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_B\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar_loss_b2a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_A\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-c1def89019c6>\u001b[0m in \u001b[0;36mcosine_distance\u001b[0;34m(real_1, real_2, fake_2)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m# test = tf.fill(tf.shape(real_1)[:-1], 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;31m# test = tf.ones(real_1.shape[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# fake_2 minus mean of X for all y in Y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "class Args():\n",
    "    dataset_dir = 'horse2zebra'\n",
    "    epoch = 200\n",
    "    epoch_step = 100\n",
    "    batch_size = 1\n",
    "    train_size = 1e8\n",
    "    load_size = 286\n",
    "    pca_components = 32\n",
    "    ngf = 64\n",
    "    ndf = 64\n",
    "    lr = 0.0002\n",
    "    beta1 = 0.5\n",
    "    which_direction = 'AtoB'\n",
    "    phase = 'train'\n",
    "    save_freq = 1000\n",
    "    print_freq = 100\n",
    "    continue_train = False\n",
    "    checkpoint_dir = './checkpoint'\n",
    "    sample_dir = './sample'\n",
    "    test_dir = './test'\n",
    "    L1_lambda = 10.0\n",
    "    use_lsgan = True\n",
    "    max_size = 50\n",
    "\n",
    "args = Args()\n",
    "\n",
    "tfconfig = tf.ConfigProto(allow_soft_placement=True)\n",
    "tfconfig.gpu_options.allow_growth = True\n",
    "with tf.Session(config=tfconfig) as sess:\n",
    "    model = cyclegan(sess, args)\n",
    "    #model.train(args) \n",
    "    #if args.phase == 'train' \\\n",
    "     #   else model.test(args)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cariGeoGAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:cariGeoGAN]",
   "language": "python",
   "name": "conda-env-cariGeoGAN-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
